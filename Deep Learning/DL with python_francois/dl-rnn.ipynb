{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "(60000, 28, 28)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 14s 232us/sample - loss: 0.3900 - acc: 0.8797 - val_loss: 0.1208 - val_acc: 0.9646\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 12s 198us/sample - loss: 0.1212 - acc: 0.9676 - val_loss: 0.0760 - val_acc: 0.9776\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 12s 198us/sample - loss: 0.0845 - acc: 0.9774 - val_loss: 0.0649 - val_acc: 0.9819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f67002a0f98>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM,CuDNNLSTM\n",
    "\n",
    "\n",
    "mnist=tf.keras.datasets.mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "\n",
    "x_train=x_train/255.0\n",
    "x_test=x_test/255.0\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "#model.add(LSTM(128,input_shape=(x_train.shape[1:]),activation='relu',return_sequences=True))\n",
    "model.add(CuDNNLSTM(128,input_shape=(x_train.shape[1:]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#model.add(LSTM(128,activation='relu'))\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "opt=tf.keras.optimizers.Adam(lr=0.001,decay=1e-6)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,epochs=3,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTC-USD\n",
      "LTC-USD\n",
      "BCH-USD\n",
      "ETH-USD\n",
      "train data: 77922 validation: 3860\n",
      "TRAIN Dont buys: 38961, buys: 38961\n",
      "VALIDATION Dont buys: 1930, buys: 1930\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import deque\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "\n",
    "SEQ_LEN=60 #length of prediction sequence to collect for rnn\n",
    "FUTURE_PERIOD_PREDICT=3  #how far into future are we going to predict\n",
    "RATIO_TO_PREDICT='LTC-USD'\n",
    "#RATIO_TO_PREDICT='BTC-USD'\n",
    "\n",
    "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  \n",
    "\n",
    "main_df=pd.DataFrame()\n",
    "for ratio in ratios:\n",
    "    print(ratio)\n",
    "    dataset=f'../input/top4bitcoinsdata/{ratio}.csv'\n",
    "    df=pd.read_csv(dataset,names=['time','low','high','open','close','volume'])\n",
    "    df.rename(columns={'close':f'{ratio}_close','volume':f'{ratio}_volume'},inplace=True)\n",
    "    df.set_index('time',inplace=True)\n",
    "    df=df[[f'{ratio}_close',f'{ratio}_volume']]\n",
    "    if len(main_df)==0:\n",
    "        main_df=df\n",
    "    else:\n",
    "        main_df=main_df.join(df)\n",
    "main_df.fillna(method='ffill',inplace=True)\n",
    "main_df.dropna(inplace=True)\n",
    "#creating target\n",
    "\n",
    "\n",
    "def classify(current,future):\n",
    "    if float(future)>float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "main_df['future']=main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "main_df['target']=list(map(classify,main_df[f'{RATIO_TO_PREDICT}_close'],main_df['future']))\n",
    "main_df[[f'{RATIO_TO_PREDICT}_close','future','target']].head(5)\n",
    "#seperating validation and train set\n",
    "times=sorted(main_df.index.values)\n",
    "last_5pcs=sorted(main_df.index.values)[-int(len(times)*0.05)]\n",
    "validation_main_df=main_df[(main_df.index>=last_5pcs)]\n",
    "main_df=main_df[(main_df.index<last_5pcs)]\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df=df.drop('future',1)\n",
    "    #scaling the data\n",
    "    for col in df.columns:\n",
    "        if col!='target':\n",
    "            df[col]=df[col].pct_change()\n",
    "            df.dropna(inplace=True)\n",
    "            df[col]=preprocessing.scale(df[col].values)\n",
    "    df.dropna(inplace=True)\n",
    "    #creating sequential data\n",
    "    sequential_data=[]\n",
    "    prev_days=deque(maxlen=SEQ_LEN)\n",
    "    \n",
    "    for i in df.values:\n",
    "        prev_days.append([n for n in i[:-1]])\n",
    "        if len(prev_days)==SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_days),i[-1]])\n",
    "    random.shuffle(sequential_data)\n",
    "    #Doing process to ensure buys and sells in equal weight\n",
    "    buys=[]\n",
    "    sells=[]\n",
    "    \n",
    "    for seq,target in sequential_data:\n",
    "        if target==0:\n",
    "            sells.append([seq,target])\n",
    "        else:\n",
    "            buys.append([seq,target])\n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "    lower=min(len(buys),len(sells))\n",
    "    \n",
    "    buys=buys[:lower]\n",
    "    sells=sells[:lower]\n",
    "    \n",
    "    sequential_data=buys+sells\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    X=[]\n",
    "    y=[]\n",
    "    for seq,target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    return np.array(X),y\n",
    "            \n",
    "            \n",
    "train_x,train_y=preprocess_df(main_df)\n",
    "validation_x,validation_y=preprocess_df(validation_main_df)\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "print(f\"TRAIN Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 77922 samples, validate on 3860 samples\n",
      "Epoch 1/10\n",
      "77922/77922 [==============================] - 22s 282us/sample - loss: 0.7112 - acc: 0.5168 - val_loss: 0.6877 - val_acc: 0.5396\n",
      "Epoch 2/10\n",
      "77922/77922 [==============================] - 20s 261us/sample - loss: 0.6872 - acc: 0.5478 - val_loss: 0.6811 - val_acc: 0.5624\n",
      "Epoch 3/10\n",
      "77922/77922 [==============================] - 20s 262us/sample - loss: 0.6821 - acc: 0.5618 - val_loss: 0.6792 - val_acc: 0.5497\n",
      "Epoch 4/10\n",
      "77922/77922 [==============================] - 20s 261us/sample - loss: 0.6793 - acc: 0.5664 - val_loss: 0.6758 - val_acc: 0.5689\n",
      "Epoch 5/10\n",
      "77922/77922 [==============================] - 21s 264us/sample - loss: 0.6783 - acc: 0.5707 - val_loss: 0.6769 - val_acc: 0.5759\n",
      "Epoch 6/10\n",
      "77922/77922 [==============================] - 21s 266us/sample - loss: 0.6759 - acc: 0.5770 - val_loss: 0.6794 - val_acc: 0.5627\n",
      "Epoch 7/10\n",
      "77922/77922 [==============================] - 21s 264us/sample - loss: 0.6727 - acc: 0.5847 - val_loss: 0.6807 - val_acc: 0.5562\n",
      "Epoch 8/10\n",
      "77922/77922 [==============================] - 21s 264us/sample - loss: 0.6678 - acc: 0.5914 - val_loss: 0.6812 - val_acc: 0.5710\n",
      "Epoch 9/10\n",
      "77922/77922 [==============================] - 20s 262us/sample - loss: 0.6615 - acc: 0.6022 - val_loss: 0.6907 - val_acc: 0.5749\n",
      "Epoch 10/10\n",
      "77922/77922 [==============================] - 21s 269us/sample - loss: 0.6521 - acc: 0.6152 - val_loss: 0.6970 - val_acc: 0.5687\n",
      "Test loss: 0.6970174242177776\n",
      "Test accuracy: 0.56865287\n"
     ]
    }
   ],
   "source": [
    "#training model\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM,CuDNNLSTM,BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ModelCheckpoint\n",
    "\n",
    "EPOCHS = 10  # how many passes through our data\n",
    "BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(CuDNNLSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "NAME=RATIO_TO_PREDICT\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "checkpoint = ModelCheckpoint(\"../input/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard,checkpoint],\n",
    ")\n",
    "\n",
    "# Score model\n",
    "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# Save model\n",
    "model.save(\"../input/{}\".format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!-- JUPYTER_TENSORBOARD_TEST_MARKER -->\n",
       "<script>\n",
       "    const req = {\n",
       "        method: 'POST',\n",
       "        contentType: 'application/json',\n",
       "        body: JSON.stringify({ 'logdir': 'logs' }),\n",
       "        headers: { 'Content-Type': 'application/json' }\n",
       "    };\n",
       "\n",
       "    const baseUrl = Jupyter.notebook.base_url;\n",
       "\n",
       "    fetch(baseUrl + 'api/tensorboard', req)\n",
       "        .then(res => res.json())\n",
       "        .then(res => {\n",
       "            const iframe = document.getElementById('tensorboard-3473771f-a261-4152-85a6-829289633742');\n",
       "            iframe.src = baseUrl + 'tensorboard/' + res.name;\n",
       "            iframe.style.display = 'block';\n",
       "        });\n",
       "</script>\n",
       "\n",
       "<iframe\n",
       "    id=\"tensorboard-3473771f-a261-4152-85a6-829289633742\"\n",
       "    style=\"width: 100%; height: 620px; display: none;\"\n",
       "    frameBorder=\"0\">\n",
       "</iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard.notebook\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
