{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#word level one hot encoding example(custom code)\nimport numpy as np\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.'] \n\ntoken_index={}\nfor sample in samples:\n    for word in sample.split():\n        if word not in token_index:\n            token_index[word]=len(token_index)+1\n\n            \nmax_length=10\n\nresults=np.zeros(shape=(len(samples),max_length,max(token_index.values())+1))\n\nprint(results.shape)\n\n\nfor i,sample in enumerate(samples):\n    for j,word in list(enumerate(sample.split()))[:max_length]:\n        index=token_index.get(word)\n        results[i,j,index]=1.\nprint(samples)\nprint(token_index)\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using keras for doing word level one hot encoding\nfrom keras.preprocessing.text import Tokenizer\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\ntokenizer=Tokenizer(num_words=1000)\ntokenizer.fit_on_texts(samples)\n\nsequences=tokenizer.texts_to_sequences(samples)\n\none_hot_results=tokenizer.texts_to_matrix(samples,mode='binary')\n\nword_index=tokenizer.word_index\n\nword_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#word embedding for imdb dataset\nfrom tensorflow.keras.datasets import imdb\nfrom keras import preprocessing\n\nmax_features=10000\nmaxlen=20\n\n(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=max_features)\n\nx_train=preprocessing.sequence.pad_sequences(x_train,maxlen=maxlen)\nx_test=preprocessing.sequence.pad_sequences(x_test,maxlen=maxlen)\n\nprint(x_train.shape) #samples,maxlen\n\n#using the embedding layer & classifier on the IMDB dataset\nfrom keras.models import Sequential\nfrom keras.layers import Flatten,Dense,Embedding\n\n\nmodel=Sequential()\nmodel.add(Embedding(10000,8,input_length=maxlen))#no_of_possible_tokens,size of embedding\nmodel.add(Flatten())\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\nmodel.summary()\n\nhistory=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using imdb dataset with pretrained embedding \nimport os\nimdb_dir='/kaggle/input/imdb-raw/aclimdb/aclImdb/'\n\ntrain_dir = os.path.join(imdb_dir, 'test')\n\nlabels = []\ntexts = []\n\nfor label_type in ['neg', 'pos']:\n    dir_name = os.path.join(train_dir, label_type)\n    for fname in os.listdir(dir_name):\n        if fname[-4:] == '.txt':\n            f = open(os.path.join(dir_name, fname))\n            texts.append(f.read())\n            f.close()\n            if label_type == 'neg':\n                labels.append(0)\n            else:\n                labels.append(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\nmaxlen=100\ntraining_samples=200\nvalidation_samples=10000\nmax_words=10000\n\ntokenizer=Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(texts)\nsequences=tokenizer.texts_to_sequences(texts)\n\nword_index=tokenizer.word_index\nprint('found %s unique tokens.'%len(word_index))\n\ndata=pad_sequences(sequences,maxlen=maxlen)\n\nlabels=np.asarray(labels)\n\nprint(data.shape)\nprint(labels.shape)\n\nindices=np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata=data[indices]\nlabels=labels[indices]\n\nx_train=data[:training_samples]\ny_train=labels[:training_samples]\nx_val=data[training_samples:training_samples+validation_samples]\ny_val=labels[training_samples:training_samples+validation_samples]\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_dir='/kaggle/input/glove6b'\nembeddings_index={}\nf=open(os.path.join(glove_dir,'glove.6B.100d.txt'))\nfor line in f:\n    values=line.split()\n    word=values[0]\n    coefs=np.asarray(values[1:],dtype='float32')\n    embeddings_index[word]=coefs\nf.close()\n\nprint(len(embeddings_index))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 100\n\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding,Flatten,Dense\nmodel=Sequential()\nmodel.add(Embedding(max_words,embedding_dim,input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()\n\nmodel.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable=False\n\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n#history=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_data=(x_val,y_val))\nhistory = model.fit(x_train, y_train,\n                    epochs=10,\n                    batch_size=32,\n                    validation_data=(x_val, y_val))\nmodel.save_weights('pretrained_glovemodel.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotCurves(history):\n    import matplotlib.pyplot as plt\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(acc) + 1)\n\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n\n    plt.figure()\n\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    plt.show()\nplotCurves(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#numpy implementation of RNN\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,SimpleRNN\ntimesteps=100\ninput_features=32\noutput_features=64\n\n\ninputs=np.random.random((timesteps,input_features))\nstate_t=np.zeros((output_features,))\n\n\nW=np.random.random((output_features,input_features))\nU=np.random.random((output_features,output_features))\nb=np.random.random((output_features,))\n\nsuccessive_outputs=[]\nfor input_t in inputs:\n    output_t=np.tanh(np.dot(W,input_t)+np.dot(U,state_t)+b)\n    \n    successive_outputs.append(output_t)\n    \n    state_t=output_t\n    \nfinal_output_sequence=np.concatenate(successive_outputs,axis=0)\n\n\n#inbuilt RNN in keras\n\nmodel=Sequential()\nmodel.add(Embedding(10000,32))\nmodel.add(SimpleRNN(32,return_sequences=True))\nmodel.add(SimpleRNN(32,return_sequences=True))\nmodel.add(SimpleRNN(32,return_sequences=True))\nmodel.add(SimpleRNN(32))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imdb datasets with RNN\nfrom tensorflow.keras.datasets import imdb\nfrom keras.preprocessing import sequence\n\nmax_features=10000\nmaxlen=500\nbatch_size=32\n\n(input_train,y_train),(input_test,y_test)=imdb.load_data(num_words=max_features)\n\nprint(len(input_train),'train sequences')\nprint(len(input_test),'test sequences')\n\nprint('pad sequences(samples*time)')\ninput_train1=sequence.pad_sequences(input_train,maxlen=maxlen)\ninput_test1=sequence.pad_sequences(input_test,maxlen=maxlen)\nprint(input_train1.shape,'train sequences')\nprint(input_test1.shape,'test sequences')\n\n\n#modelling\nfrom keras.models import Sequential\nfrom keras.layers import Flatten,Dense,Embedding,SimpleRNN\nmodel=Sequential()\nmodel.add(Embedding(max_features,32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\nhistory=model.fit(input_train1,y_train,epochs=10,batch_size=128,validation_split=0.2)\n\nplotCurves(history)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using the LSTM layers in keras\nfrom keras.layers import Flatten,Dense,Embedding,SimpleRNN,LSTM\nmodel=Sequential()\nmodel.add(Embedding(max_features,32))\nmodel.add(LSTM(32))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\nhistory=model.fit(input_train1,y_train,epochs=10,batch_size=128,validation_split=0.2)\nplotCurves(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using RNN for time series data\n!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n!unzip jena_climate_2009_2016.csv.zip\nimport os\nos.listdir('/kaggle/working')\ndata_dir='/kaggle/working/'\nfname=os.path.join(data_dir,'jena_climate_2009_2016.csv')\nf=open(fname)\ndata=f.read()\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lines=data.split('\\n')\nheader=lines[0].split(',')\nprint(header)\nlines=lines[1:]\nprint(lines[1:2])\nprint(len(lines))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfloat_data=np.zeros((len(lines),len(header)-1))\nfor i,line in enumerate(lines):\n    values=[float(x) for x in line.split(',')[1:]]\n    float_data[i,:]=values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting temp time series\nfrom matplotlib import pyplot as plt\ntemp=float_data[:,1]\nplt.plot(range(len(temp)),temp)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#144 points per day (6 per hour) for 10 days\nplt.plot(range(1440),temp[:1440])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#given data going as far back as lookback timesteps (a timestep is 10 minutes) and sampled every steps timesteps,\n#can you predict the temperature in delay timesteps?\n#lookback = 1440—Observations will go back 10 days.\n#steps = 6—Observations will be sampled at one data point per hour.\n#delay = 144—Targets will be 24 hours in the future.\n#normalizing the data first\nmean=float_data[:200000].mean(axis=0)\nstd=float_data[:200000].std(axis=0)\nfloat_data-=mean\nfloat_data/=std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generator(data,lookback,delay,min_index,max_index,shuffle=False,batch_size=128,step=6):\n    if max_index is None:\n        max_index=len(data)-delay-1\n    i=min_index+lookback\n    while 1:\n        if shuffle:\n            rows=np.random.randint(min_index+lookback,max_index,size=batch_size)\n            print(rows.shape)\n        else:\n            if i+batch_size>=max_index:\n                i=min_index+lookback\n            rows=np.arange(i,min(i+batch_size,max_index))\n            i+=len(rows)\n            \n        samples=np.zeros((len(rows),lookback//step,data.shape[-1]))\n        targets=np.zeros((len(rows),))\n        \n        for j, row in enumerate(rows):\n            indices = range(rows[j] - lookback, rows[j], step)\n            samples[j] = data[indices]\n            targets[j] = data[rows[j] + delay][1]\n        yield samples,targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lookback=1440\nstep=6\ndelay=144\nbatch_size=128\ntrain_gen=generator(float_data,lookback=lookback,delay=delay,min_index=0,max_index=200000,shuffle=True,step=step,batch_size=batch_size)\nval_gen = generator(float_data,\n                    lookback=lookback,\n                    delay=delay,\n                    min_index=200001,\n                    max_index=300000,\n                    step=step,\n                    batch_size=batch_size)\ntest_gen = generator(float_data,\n                     lookback=lookback,\n                     delay=delay,\n                     min_index=300001,\n                     max_index=None,\n                     step=step,\n                     batch_size=batch_size)\nval_steps=(300000-200001-lookback)//batch_size\ntes_steps = (len(float_data) - 300001 - lookback) // batch_size\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#naive baseline\ndef evaluate_naive_method():\n    batch_maes=[]\n    for step in range(val_steps):\n        samples,targets=next(val_gen)\n        preds=samples[:,-1,1]\n        mae=np.mean(np.abs(preds-targets))\n        batch_maes.append(mae)\n    print(np.mean(batch_maes))\nevaluate_naive_method()\ncelsius_mae=0.29*std[1]\ncelsius_mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#first training with a cheap model\n#input is of the form lookback//step,data.shape[-1]\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\n\nmodel=Sequential()\nmodel.add(layers.Flatten(input_shape=(lookback//step,float_data.shape[-1])))\nmodel.add(layers.Dense(32,activation='relu'))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(),loss='mae')\nhistory=model.fit_generator(train_gen,steps_per_epoch=500,epochs=20,validation_data=val_gen,validation_steps=val_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotloss(history):\n    import matplotlib.pyplot as plt\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(loss) + 1)\n\n    plt.figure()\n\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    plt.show()\nplotloss(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using GRU\nmodel=Sequential()\nmodel.add(layers.GRU(32,input_shape=(None,float_data.shape[-1])))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=RMSprop(),loss='mae')\nhistory = model.fit_generator(train_gen,\n                              steps_per_epoch=500,\n                              epochs=20,\n                              validation_data=val_gen,\n                              validation_steps=val_steps)\nplotloss(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training and evaluating dropout stacked GRU model\nmodel=Sequential()\nmodel.add(layers.GRU(32,dropout=0.1,recurrent_dropout=0.5,return_sequences=True,input_shape=(None,float_data.shape[-1])))\nmodel.add(layers.GRU(64,activation='relu',dropout=0.1,recurrent_dropout=0.5))\nmodel.add(layers.Dense(1))\n\nmodel.compile(optimizer=RMSprop(), loss='mae')\nhistory = model.fit_generator(train_gen,\n                              steps_per_epoch=500,\n                              epochs=10,\n                              validation_data=val_gen,\n                              validation_steps=val_steps)\nplotloss(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bidirectional LSTM\nmodel=Sequential()\nmodel.add(layers.Embedding(100,32))\nmodel.add(layers.Bidirectional(layers.LSTM(32)))\nmodel.add(layers.Dense(1,activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}